{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619283b1",
   "metadata": {},
   "source": [
    "# Pricing simulation with Q-learning agents in Julia\n",
    "\n",
    "This is a Julia implementation of a simulation study in which Q-learning agents compete in a market environment with\n",
    "inelastic demand.\n",
    "\n",
    "\n",
    "# The market environment\n",
    "The market environment is described in detail in the paper [\"Algorithmic and Human Collusion\"](https://tofewe.github.io/Algorithmic_and_Human_Collusion_Tobias_Werner.pdf).\n",
    "\n",
    "In short: \n",
    "- Price competition with homogeneous goods and no production costs in a repeated game\n",
    "- Market consists of $N\\in \\{2,3\\}$ firms \n",
    "- Firms are represented by a Q-learning algorithm\n",
    "- 60 computerized consumers with perfectly inelastic unit demand and a reservation price of $p^{max}=4$\n",
    "- In each period $t$, firms set prices simultaneously $p_i^t\\in \\mathscr{P} := \\{0,... , 5\\}$\n",
    "- Market is shared in each round among the firms with the lowest price\n",
    "\n",
    "However, the environment can be customized easily (see below).\n",
    "\n",
    "The profit function `calc_reward` computes the profits in each period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e9504",
   "metadata": {},
   "source": [
    "# Defining the parameters of the simulation\n",
    "The DataType `SimParams` from the `sim_types.jl` module defines the main parameters that describe the simulation setup. See below for a documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71e251a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimParams"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"sim_types.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be7ff7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "The parameters required for the simulation.\n",
       "\n",
       "\\section{Fields:}\n",
       "\\begin{itemize}\n",
       "\\item n\\_firms: Number of firms in the market\n",
       "\n",
       "\n",
       "\\item min\\_price: Minimal price\n",
       "\n",
       "\n",
       "\\item max\\_price: Maximal price\n",
       "\n",
       "\n",
       "\\item reservation\\_price: reservation price of the consumers\n",
       "\n",
       "\n",
       "\\item m\\_consumer: Number of consumers in the market\n",
       "\n",
       "\n",
       "\\item k\\_memory: Memory of the agent\n",
       "\n",
       "\n",
       "\\item beta: Exploration decay of the agent\n",
       "\n",
       "\n",
       "\\item discount\\_rate: Discount rate as in any economic model\n",
       "\n",
       "\n",
       "\\item alpha: Learning rate\n",
       "\n",
       "\n",
       "\\item max\\_t: Maximal number of learning iterations\n",
       "\n",
       "\n",
       "\\item rounds\\_convergence: Number of rounds to determine convergence\n",
       "\n",
       "\n",
       "\\item optimality\\_threshold: Threshold to determine convergence for q*\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "The parameters required for the simulation.\n",
       "\n",
       "# Fields:\n",
       "\n",
       "  * n_firms: Number of firms in the market\n",
       "  * min_price: Minimal price\n",
       "  * max_price: Maximal price\n",
       "  * reservation_price: reservation price of the consumers\n",
       "  * m_consumer: Number of consumers in the market\n",
       "  * k_memory: Memory of the agent\n",
       "  * beta: Exploration decay of the agent\n",
       "  * discount_rate: Discount rate as in any economic model\n",
       "  * alpha: Learning rate\n",
       "  * max_t: Maximal number of learning iterations\n",
       "  * rounds_convergence: Number of rounds to determine convergence\n",
       "  * optimality_threshold: Threshold to determine convergence for q*\n"
      ],
      "text/plain": [
       "  The parameters required for the simulation.\n",
       "\n",
       "\u001b[1m  Fields:\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    n_firms: Number of firms in the market\n",
       "\n",
       "    •    min_price: Minimal price\n",
       "\n",
       "    •    max_price: Maximal price\n",
       "\n",
       "    •    reservation_price: reservation price of the consumers\n",
       "\n",
       "    •    m_consumer: Number of consumers in the market\n",
       "\n",
       "    •    k_memory: Memory of the agent\n",
       "\n",
       "    •    beta: Exploration decay of the agent\n",
       "\n",
       "    •    discount_rate: Discount rate as in any economic model\n",
       "\n",
       "    •    alpha: Learning rate\n",
       "\n",
       "    •    max_t: Maximal number of learning iterations\n",
       "\n",
       "    •    rounds_convergence: Number of rounds to determine convergence\n",
       "\n",
       "    •    optimality_threshold: Threshold to determine convergence for q*"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?SimParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb3c38",
   "metadata": {},
   "source": [
    "We use the parameters from the paper but it can be easily adjusted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06bbd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimParams(2, 0, 5, 4, 60, 1, 4.0e-7, 0.95, 0.04, 1000000000, 100000, 1.0e-11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_firms = 2\n",
    "min_price = 0\n",
    "max_price = 5\n",
    "reservation_price = 4\n",
    "m_consumer = 60\n",
    "k_memory = 1\n",
    "beta = 4e-07\n",
    "discount_rate = 0.95\n",
    "alpha = 0.04\n",
    "max_t = 1000000000\n",
    "rounds_convergence = 100000\n",
    "optimality_threshold = 0.00000000001\n",
    "\n",
    "current_params = SimParams(n_firms = n_firms,\n",
    "                           min_price = min_price,\n",
    "                           max_price = max_price,\n",
    "                           reservation_price = reservation_price,\n",
    "                           m_consumer = m_consumer,\n",
    "                           k_memory = k_memory,\n",
    "                           beta = beta,\n",
    "                           discount_rate = discount_rate,\n",
    "                           alpha = alpha,\n",
    "                           max_t = max_t,\n",
    "                           rounds_convergence = rounds_convergence,\n",
    "                           optimality_threshold=optimality_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6eef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calc_reward"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"market_env.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adf34927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1m_\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mw\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1md\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Return the profit for the firm in the market.\n",
       "\n",
       "\\section{Arguments:}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{price::Integer}: Price of the firm\n",
       "\n",
       "\n",
       "\\item \\texttt{all\\_prices::Array\\{Integer,1\\}}: All prices in the market in the given round\n",
       "\n",
       "\n",
       "\\item \\texttt{p::SimParams}: Market parameter\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "Return the profit for the firm in the market.\n",
       "\n",
       "# Arguments:\n",
       "\n",
       "  * `price::Integer`: Price of the firm\n",
       "  * `all_prices::Array{Integer,1}`: All prices in the market in the given round\n",
       "  * `p::SimParams`: Market parameter\n"
      ],
      "text/plain": [
       "  Return the profit for the firm in the market.\n",
       "\n",
       "\u001b[1m  Arguments:\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    \u001b[36mprice::Integer\u001b[39m: Price of the firm\n",
       "\n",
       "    •    \u001b[36mall_prices::Array{Integer,1}\u001b[39m: All prices in the market in the\n",
       "        given round\n",
       "\n",
       "    •    \u001b[36mp::SimParams\u001b[39m: Market parameter"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?calc_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d418c28",
   "metadata": {},
   "source": [
    "# Q-learning agents in the market environment\n",
    "\n",
    "We have to make some assumption how algorithms perceive the market environment:\n",
    "- The action set given by the set of possible prices $\\mathbf{A} \\equiv \\mathscr{P}$\n",
    "    - Makes inutitive sense since we look at pricing algorithms\n",
    "- State $s_t$ is the combination of past prices $s_t = \\{p^{t-1}_1, ..., p^{t-1}_N\\}$\n",
    "    - Boils down to memory-one strategies predominantly used by humans in PD\n",
    "- Reward signal is given by the economic profit $\\pi^t_i$\n",
    "\n",
    "The main function are implemented in the `agent.jl` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd9e2970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calc_epsilon"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"agent.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fe8e3",
   "metadata": {},
   "source": [
    "# Training simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e102b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_transition_dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils_simulation.jl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "692097bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_agent (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_agent(seed::Int64, p::SimParams)\n",
    "    # Create local random number generator\n",
    "    local_RNG = MersenneTwister(seed)\n",
    "    \n",
    "    # Create variable from the parameters p\n",
    "    all_prices, all_price_states, n_states, n_actions = create_vars(p)\n",
    "\n",
    "    # Create mapping dicts from int states (column index) to prices and vice versa\n",
    "    int_to_price_state, price_state_to_int = create_transition_dict(all_price_states, n_states)\n",
    "    # Create Q-matrices\n",
    "    all_q = [rand(local_RNG, Float64, n_states, n_actions) for _ in 1:p.n_firms]\n",
    "    \n",
    "    # Start with the training\n",
    "    # Arbitraty start stae\n",
    "    state = 1\n",
    "    next_state = 1\n",
    "    # Init convergence counter \n",
    "    convergence_count = 0\n",
    "    \n",
    "    # Init best action\n",
    "    best_actions = []\n",
    "    \n",
    "    \n",
    "    for t in 1:p.max_t\n",
    "        epsilon = calc_epsilon(t, beta)\n",
    "        best_actions_old = copy(best_actions)\n",
    "        current_actions = tuple([get_action(q, state, epsilon, n_actions, local_RNG) for q in all_q]...)\n",
    "        current_prices = collect(index_to_price(current_actions)) #Collect to get an array\n",
    "        current_profits = [calc_reward(price, current_prices, p) for price in current_prices]\n",
    "        next_state = price_state_to_int[tuple(current_prices...)] #Cast to tuple again due to hash\n",
    "\n",
    "        # Get best actions before the update\n",
    "        best_actions_before_update = [get_best_action(q, state) for q in all_q]\n",
    "\n",
    "        # Update\n",
    "        all_q = [update(q, state, action, reward, next_state, p) for (q, action, reward) in zip(all_q, current_actions, current_profits)]\n",
    "\n",
    "        # Get best actions after the update, Note that only the best action for the given \n",
    "        # state could have changed\n",
    "        best_action_after_update = [get_best_action(q, state) for q in all_q]\n",
    "\n",
    "        # check convergence\n",
    "        if best_actions_before_update == best_action_after_update\n",
    "            convergence_count += 1\n",
    "            if convergence_count > p.rounds_convergence\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            convergence_count = 0\n",
    "        end\n",
    "        # State is the new state from the last period\n",
    "        state = next_state\n",
    "    end    \n",
    "    return state, all_q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb14e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34.450283 seconds (335.78 M allocations: 24.849 GiB, 11.29% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29, [[1648.8671342423718 2080.8463277747783 … 1731.3639410891965 1732.9709092358225; 1919.2879404227992 1923.8881296448887 … 2031.3187115020885 1903.442959982311; … ; 2011.0488630223022 2020.9026847747546 … 2035.9271157486503 1927.7129222726355; 1719.95837213905 2137.8690134186572 … 1865.750459160043 1753.2403060634565], [1656.935474509337 1670.895865518859 … 1639.39630813921 1687.3168718941777; 1885.1706516508577 1927.5296435923344 … 1840.7675616280355 1895.7199627028772; … ; 2052.0615171194927 2210.0951786637816 … 2137.4965740441394 2059.0308552001866; 1705.3580438008694 1694.3219286948456 … 2163.721003912153 1682.1541203531033]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time convergence_state, all_q =  train_agent(6, current_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.0-1 1.5.0",
   "language": "julia",
   "name": "juliapro_v1.5.0-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
